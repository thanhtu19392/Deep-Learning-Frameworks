{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro to Deep Learning Frameworks.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"toc_visible":true}},"cells":[{"metadata":{"id":"Rfz4I_H75Clz","colab_type":"text"},"source":["\n","\n","## Introduction to Deep Learning Frameworks\n","\n","---\n","\n","\n","\n","\n","\n"],"cell_type":"markdown"},{"metadata":{"id":"6czyrnnlcTQL","colab_type":"text"},"source":["\n","### Why using deep learning frameworks?\n","\n","- Scales machine learning code\n","- Computes gradients!\n","- Standardizes machine learning applications for sharing\n","- Zoo of Deep Learning frameworks available with different advantages, paradigms, levels of abstraction, programming languages...\n","- Interface with GPU for parallel processing\n","\n","**What is Tensorflow?** \n","- Open source software library for numerical computation using data flow graphs\n","- Originally developed by Google Brain Team to conduct machine learning research\n","- Tensorflow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms\n","\n","Big idea: express a numeric computation as a graph\n","- graph nodes are operations which have any number of inputs and outputs\n","- graph edges are tensors which flow between nodes"],"cell_type":"markdown"},{"metadata":{"id":"6pmqc6BYoSlM","colab_type":"text"},"source":["### Tensors\n","Tensors are the primary data structure to operate on the computational graph.\n","Main ways to create tensors in Tensorflow: \n","1. **Fixed tensors:**\n","  - Create a zero filled tensor: \n","  ```zero_tsr = tf.zeros([row_dim, col_dim]) ```\n","  - Create a one filled tensor: \n","  ```ones_tsr = tf.ones([row_dim, col_dim])```\n","  - Create a constant filled tensor: \n","  ```filled_tsr = tf.fill([row_dim, col_dim], 42)```\n","  - Create a tensor out of an existing constant: \n","  ```constant_tsr = tf.constant([1,2,3])```\n","\n","2. **Tensors of similar shape:  **   \n","    ```zeros_similar = tf.zeros_like(constant_tsr) ```     \n","    ```ones_similar = tf.ones_like(constant_tsr)```\n","  \n","3. **Sequence tensors** (tensors contain defined intervals):     \n","  - ```linear_tsr = tf.linspace(start=0, stop=1, start=3) ```\n","  - ```integer_seq_tsr = tf.range(start=6, limit=15, delta=3)```\n","4. **Random tensors: **\n","  - uniform distribution: ```randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)```\n","  - normal distribution: ```randnorm_tsr = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)```\n","  - truncated_normal() function picks normal values within two standard deviations of the specified mean: ```runcnorm_tsr = tf.truncated_normal([row_dim, col_dim],\n","mean=0.0, stddev=1.0)```\n","5. **randomizing entries of arrays.** \n","  - ```shuffled_output = tf.random_shuffle(input_tensor)```\n"],"cell_type":"markdown"},{"metadata":{"id":"2nFqVOG8sXbM","colab_type":"text"},"source":["### Placeholders and Variables\n","**Variables** are the *parameters* of the algorithm and TensorFlow keeps track of how to change these to optimize the algorithm.     \n","\n","**Placeholders** are objects that allow you to feed in data of a specifc type and shape and depend on the results of the computational graph, such as the expected outcome of a computation.\n","\n","1. create a variable: \n","    - declare a variable: variable = tf.Variable(tensor)    \n","```Ex: my_var = tf.Variable(tf.zeros([2,3])```      \n","    - initialize the variable:     \n","    ```sess = tf.Session()```     \n","```initialize_op = tf.global_variables_initializer ()```    \n","```sess.run(initialize_op)```\n","\n","2. Placeholders:  \n","  - assign data type, assign a shape of a tensor   \n","  ```x = tf.placeholder(tf.float32, shape=[2,2])```\n","  - get data from a feed_dict argument in the session. \n","  ``` y = tf.identity(x) -> identity operation```\n","  ``` x_vals = np.random.rand(2,2)```\n","  ``` sess.run(y, feed_dict={x: x_vals})```"],"cell_type":"markdown"},{"metadata":{"id":"ZKMqiUE9v2bv","colab_type":"text"},"source":["### Operations\n","\n","1. standard operations on tensors: add(), sub(), mul(), and div().\n","2. tf.abs() : absolute value of one input tensor\n","3. tf.ceil() : Ceiling funtion\n","4. tf.maximum(): Element-wise max of two tensors\n","...."],"cell_type":"markdown"},{"metadata":{"id":"DBnp7Jbgwj0K","colab_type":"text"},"source":["### Activation Functions\n","\n","Activation functions are non-linear operations that act on tensors    \n","\n","1. ReLU = max(0,x):     \n","```tf.nn.relu([-3., 3., 10.])```\n","2. ReLU6 = min(max(0,x),6):     \n","```tf.nn.relu6([-3., 3., 10.])```\n","3. Sigmoid function = 1/(1+exp(-x)) :    \n","```tf.nn.sigmoid([-1., 0., 1.])```\n","4. tanh = ((exp(x)- exp(-x))/(exp(x)+exp(-x)):      \n","```tf.nn.tanh([-1., 0., 1.])```\n"],"cell_type":"markdown"},{"metadata":{"id":"3jSMWwi_ygPi","colab_type":"text"},"source":["### Loss Functions\n","\n","#### 1. Loss Functions for regression: \n","  - **L2 norm** (Euclidean Loss function): square of the distance to the target      \n","  ```l2_y_vals = tf.square(target - x_vals)```\n","  ```l2_y_out = sess.run(l2_y_vals)```\n","  \n","  - **L1 norm** (Absolute loss function): instead of squaring the difference, we take the absolute value    \n","  ```l1_y_vals = tf.abs(target - x_vals)```\n","  \n","#### 2. Loss function for classification:     \n","  - **Hinge loss**: compute a loss between with two target classes, 1 and -1      \n","  ```hinge_y_vals = tf.maximum(0., 1. - tf.mul(target, x_vals))```\n","\n","  - **Cross-entropy loss** (logistic loss): measure a distance from the actual class (0 or 1) to the predicted value, which is usually a real number between 0 and 1.    ```xentropy_y_vals = - tf.mul(target, tf.log(x_vals)) - tf.mul((1. -target), tf.log(1. - x_vals))```\n","  \n","  - **Sigmoid cross entropy**:     \n","  ```xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(x_vals, targets)```\n","  \n","  - **Weighted cross-entropy loss**:weighted version of the sigmoid cross entropy loss. We provide a weight on the positive target     \n","  ```weight = tf.constant(0.5)```     \n","```xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals, targets, weight)```\n","\n","  - **Softmax cross-entropy loss**: measure a loss when there is only one target category instead of multiple.  the function transforms the outputs into a probability distribution via the softmax function and then computes the loss function from a true probability distribution.     \n","  ```unscaled_logits = tf.constant([[1., -3., 10.]])```     \n","  ```target_dist = tf.constant([[0.1, 0.02, 0.88]])```      \n","  ```softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits(unscaled_logits, target_dist)```\n","  \n","  "],"cell_type":"markdown"},{"metadata":{"id":"zOBsZpgZ8Al9","colab_type":"text"},"source":["### Implementing Backpropagation\n","\n","1. Created the data.\n","2. Initialized placeholders and variables.\n","3. Created a loss function.\n","4. Defned an optimization algorithm.\n","5. And fnally, iterated across random data samples to iteratively update our variables\n","\n","```python\n","loss = tf.square(my_output - y_target)\n","my_opt = tf.train.GradientDescentOptimizer(learning_rate=0.02)\n","train_step = my_opt.minimize(loss)```\n","\n","Another optimization algorithm:    \n","  - MomentumOptimizer()\n","  - AdagradOptimizer()\n","  - AdadeltaOptimizer()\n","  - AdamOptimizer()    \n","  ...."],"cell_type":"markdown"},{"metadata":{"id":"SvcOfEW3ePaN","colab_type":"text"},"source":["Example: we want to compute ReLU activation \n","\n","![ReLU activation](https://cdn-images-1.medium.com/max/1600/1*G9MXGOM2jWl3SOXuqRlV3A.png)\n","\n","Three principal type of node in tensorflow:    \n","- **Variables** are stateful nodes which output their current value **(W, b)**.   \n","- **Placeholders** are nodes whose value is fed in at execution time. We don't give any initial values, just assign a data type, and we assign a shape of a tensor so the graph still knows what to compute even though it doesn't have any stored values yet **(x)**.     \n","- **Mathematical operations**: ex **MatMul, Add, ReLU**"],"cell_type":"markdown"},{"metadata":{"id":"hZb8ZTnabjFS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# create graph - backbone of our model\n","import tensorflow as tf\n","import numpy as np\n","\n","b = tf.Variable(tf.zeros((100,)))\n","W = tf.Variable(tf.random_uniform((784,100), -1 , 1))\n","\n","x = tf.placeholder(tf.float32, (100,784))\n","\n","h = tf.nn.relu(tf.matmul(x,W) + b)"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"nUVtrX_QinQH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":238},"outputId":"16c742f9-8cbe-4441-c0c3-f5f040bf6d98","executionInfo":{"status":"ok","timestamp":1513633201267,"user_tz":-60,"elapsed":545,"user":{"displayName":"Nguyen Thanh Tu","photoUrl":"//lh5.googleusercontent.com/-c4iQgKRaDhU/AAAAAAAAAAI/AAAAAAAACbc/r_zQiZxk3qs/s50-c-k-no/photo.jpg","userId":"111258210471892593308"}}},"source":["# create a session and fit our numerical input to our graph\n","sess = tf.Session()\n","sess.run(tf.initialize_all_variables())\n","sess.run(h, {x: np.random.rand(100, 784)})"],"cell_type":"code","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0.        ,   9.38900852,   8.5437212 , ...,   0.        ,\n","          2.11395288,  11.79295635],\n","       [  0.        ,  11.15999031,   7.56981277, ...,   0.        ,\n","          6.30800915,  12.28312874],\n","       [  0.        ,   8.55609322,   5.16542864, ...,   0.        ,\n","          5.78558874,  10.00546741],\n","       ..., \n","       [  0.        ,   7.19157362,   9.9484663 , ...,   5.70030022,\n","          3.15153456,   0.        ],\n","       [  0.        ,   8.64635563,   2.45155334, ...,   3.20237422,\n","          0.19523752,   6.6025753 ],\n","       [  0.        ,  17.83238029,   4.84830666, ...,   0.        ,\n","          0.        ,   9.50206089]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"xTcn8mJej-p0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# create loss function\n"],"cell_type":"code","execution_count":0,"outputs":[]}]}